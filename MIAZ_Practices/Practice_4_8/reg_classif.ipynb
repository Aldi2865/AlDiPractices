{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Імпорт бібліотек\n",
    "\n",
    "У цій комірці ми імпортуємо необхідні бібліотеки для нашої роботи:\n",
    "\n",
    "*   **`tensorflow` (`tf`):** Основна бібліотека для глибокого навчання, яка надає інструменти для створення та навчання нейронних мереж.\n",
    "*   **`tensorflow.keras` (`keras`):** Високорівневий API для TensorFlow, що спрощує процес побудови та навчання моделей.\n",
    "*   **`sklearn.datasets.make_regression`:** Функція для генерації синтетичного набору даних для задачі регресії.\n",
    "*   **`sklearn.datasets.make_classification`:** Функція для генерації синтетичного набору даних для задачі класифікації.\n",
    "*   **`sklearn.model_selection.train_test_split`:** Функція для розділення даних на тренувальний та тестовий набори.\n",
    "*   **`matplotlib.pyplot` (`plt`):** Бібліотека для створення графіків та візуалізації даних.\n",
    "*   **`numpy` (`np`):** Бібліотека для роботи з числовими масивами та математичними операціями.\n",
    "*   **`sklearn.metrics.mean_squared_error`:** Функція для розрахунку середньоквадратичної помилки (MSE), яка використовується для оцінки моделей регресії.\n",
    "*   **`sklearn.metrics.accuracy_score`:** Функція для розрахунку точності (accuracy), яка використовується для оцінки моделей класифікації.\n",
    "*   **`sklearn.metrics.confusion_matrix`:** Функція для побудови матриці плутанини (confusion matrix), яка використовується для оцінки моделей класифікації.\n",
    "*   **`seaborn` (`sns`):** Бібліотека для візуалізації статистичних даних, зокрема, зручно використовувати для візуалізації матриці плутанини.\n",
    "\n",
    "Ці бібліотеки надають нам всі необхідні інструменти для створення, навчання, оцінки та візуалізації моделей машинного навчання в даному Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Імпортуємо бібліотеку TensorFlow для роботи з нейронними мережами\n",
    "from tensorflow import keras  # Імпортуємо Keras, високорівневий API для TensorFlow\n",
    "from sklearn.datasets import make_regression, make_classification  # Імпортуємо функції для генерації даних\n",
    "from sklearn.model_selection import train_test_split  # Імпортуємо функцію для розділення даних на тренувальні та тестові\n",
    "import matplotlib.pyplot as plt  # Імпортуємо бібліотеку для візуалізації даних\n",
    "import numpy as np  # Імпортуємо бібліотеку NumPy для роботи з масивами\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix  # Імпортуємо метрики для оцінки моделей\n",
    "import seaborn as sns  # Імпортуємо бібліотеку Seaborn для візуалізації матриці плутанини"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ініціалізація змінної `student_id`\n",
    "\n",
    "У цій комірці ми ініціалізуємо змінну `student_id`, яка буде використовуватись протягом усього ноутбука для налаштування параметрів генерації даних та конфігурації моделей.\n",
    "\n",
    "**Значення `student_id` повинно відповідати вашому номеру за журналом.** Це дозволить персоналізувати завдання та отримати результати, що відповідають нашому варіанту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student_id = 3  # Мій номер а журналом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Генерація даних\n",
    "\n",
    "У цьому розділі ми генеруємо синтетичні дані, які будуть використовуватись для навчання та тестування моделей машинного навчання. Ми створимо два набори даних: один для задачі **регресії**, а інший - для задачі **класифікації**.\n",
    "\n",
    "### 3.1. --- Регресія ---\n",
    "\n",
    "Для генерації даних для регресії ми використовуємо функцію `make_regression` з бібліотеки `sklearn.datasets`. Ця функція створює дані з лінійною залежністю між вхідними ознаками та вихідною змінною, додаючи при цьому певний рівень шуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Генерація даних\n",
    "\n",
    "# --- Регресія ---\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000 * student_id,  # Кількість зразків даних (1000 помножено на номер за журналом)\n",
    "    n_features=1,  # Кількість ознак (вхідних змінних) для кожного зразка (в даному випадку 1)\n",
    "    noise=10,  # Стандартне відхилення Гаусівського шуму, доданого до вихідних даних\n",
    "    # --- інші параметри ---\n",
    "    # bias=0.0,  # Зсув лінії регресії\n",
    "    # coef=False,  # Чи повертати коефіцієнти регресії (True/False)\n",
    "    # effective_rank=None,  # Ефективний ранг матриці ознак\n",
    "    # tail_strength=0.5,  # Сила \"хвостів\" розподілу помилок\n",
    "    # random_state=None  # Фіксоване значення для генератора випадкових чисел\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. --- Класифікація ---\n",
    "\n",
    "Для генерації даних для класифікації ми використовуємо функцію `make_classification` з бібліотеки `sklearn.datasets`. Ця функція створює дані з декількома класами, де кожен клас може бути представлений одним або декількома кластерами точок у багатовимірному просторі ознак.\n",
    "\n",
    "**Параметри функції `make_classification`:**\n",
    "\n",
    "*   **`n_samples`:** Кількість зразків даних. Як і для регресії, ми генеруємо `1000 * student_id` зразків.\n",
    "*   **`n_features`:** Кількість ознак (вхідних змінних) для кожного зразка. Ми встановили значення `n_features=4`, тобто кожен зразок буде мати 4 ознаки.\n",
    "*   **`n_informative`:** Кількість інформативних ознак. Це ті ознаки, які дійсно впливають на приналежність зразка до певного класу. Ми встановили `n_informative=4`, тобто всі згенеровані ознаки є інформативними.\n",
    "*   **`n_redundant`:** Кількість надлишкових ознак. Це ознаки, які є лінійними комбінаціями інформативних ознак. Ми встановили `n_redundant=0`, тобто надлишкових ознак немає.\n",
    "*   **`n_repeated`:** Кількість повторюваних ознак. Це ознаки, які є точними копіями інших ознак. Ми встановили `n_repeated=0`, тобто повторюваних ознак немає.\n",
    "*   **`n_classes`:** Кількість класів. Ми генеруємо `student_id + 2` класів, тобто кількість класів залежить від вашого номера за журналом.\n",
    "*   **`n_clusters_per_class`:** Кількість кластерів на кожен клас. Ми встановили `n_clusters_per_class=1`, тобто кожен клас буде представлений одним кластером.\n",
    "*   **`weights`:** Список ваг для кожного класу. Якщо `None`, всі класи мають однакову вагу. Ми встановили `weights=None`.\n",
    "*   **`flip_y`:** Відсоток міток, які будуть випадково змінені. Це додає шум у дані і робить задачу класифікації складнішою. Ми встановили `flip_y=0.01`, тобто 1% міток буде змінено.\n",
    "*   **`class_sep`:** Коефіцієнт, що контролює відстань між кластерами. Більше значення `class_sep` призводить до більш чітко розділених класів. Ми встановили `class_sep=1.0`.\n",
    "*   **`hypercube`:** Якщо `True`, кластери будуть розташовані у вершинах гіперкуба. Якщо `False`, кластери будуть розташовані у вершинах випадкового багатогранника. Ми встановили `hypercube=True`.\n",
    "*   **`shift`:** Зсуває значення ознак на задане значення. Ми встановили `shift=0.0`.\n",
    "*   **`scale`:** Масштабує значення ознак на задане значення. Ми встановили `scale=1.0`.\n",
    "*   **`shuffle`:** Якщо `True`, зразки та ознаки будуть перемішані. Ми встановили `shuffle=True`.\n",
    "*   **`random_state`:** Фіксоване значення для генератора випадкових чисел. Забезпечує відтворюваність результатів при повторному запуску коду. Ми встановили `random_state=42`.\n",
    "\n",
    "**Результат:**\n",
    "\n",
    "Функція `make_classification` повертає два масиви:\n",
    "\n",
    "*   **`X_class`:** Масив NumPy розміром `(n_samples, n_features)`, що містить вхідні ознаки.\n",
    "*   **`y_class`:** Масив NumPy розміром `(n_samples,)`, що містить мітки класів (цілочисельні значення від 0 до `n_classes - 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Класифікація ---\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000 * student_id,  # Кількість зразків даних (1000 помножено на номер за журналом)\n",
    "    n_features=4,  # Кількість ознак (вхідних змінних) для кожного зразка (в даному випадку 4)\n",
    "    n_informative=4,  # Кількість інформативних ознак, які фактично використовуються для генерації даних\n",
    "    n_redundant=0,  # Кількість надлишкових ознак, які є лінійними комбінаціями інформативних ознак\n",
    "    n_repeated=0,  # Кількість повторюваних ознак, які є дублікатами інформативних або надлишкових ознак\n",
    "    n_classes=student_id + 2,  # Кількість класів (номер за журналом + 2)\n",
    "    n_clusters_per_class=1,  # Кількість кластерів на клас\n",
    "    class_sep=1.0,  # Відстань між кластерами\n",
    "    flip_y=0.01,  # Відсоток міток, які будуть випадково змінені\n",
    "    random_state=42  # Фіксоване значення для генератора випадкових чисел\n",
    "    # weights=None,  # Список ваг для кожного класу (None - рівні ваги) - значення за замовчуванням\n",
    "    # hypercube=True,  # Чи генерувати дані в гіперкубі - значення за замовчуванням\n",
    "    # shift=0.0,  # Зсув даних - значення за замовчуванням\n",
    "    # scale=1.0,  # Масштабування даних - значення за замовчуванням\n",
    "    # shuffle=True,  # Чи перемішувати дані - значення за замовчуванням\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Візуалізація ---\n",
    "plt.figure(figsize=(12, 5))  # Створюємо фігуру для графіків розміром 12x5 дюймів\n",
    "\n",
    "plt.subplot(1, 2, 1)  # Створюємо перший підграфік (1 рядок, 2 стовпці, 1-й графік)\n",
    "plt.scatter(\n",
    "    X_reg,  # Дані для осі X\n",
    "    y_reg,  # Дані для осі Y\n",
    "    s=10,  # Розмір точок\n",
    "    c='blue',  # Колір точок\n",
    "    marker='o',  # Форма маркерів (точки)\n",
    "    alpha=0.5  # Прозорість точок\n",
    ")  \n",
    "plt.title('Дані для регресії')  # Додаємо заголовок\n",
    "plt.xlabel('X')  # Додаємо підпис осі X\n",
    "plt.ylabel('y')  # Додаємо підпис осі Y\n",
    "\n",
    "plt.subplot(1, 2, 2)  # Створюємо другий підграфік (1 рядок, 2 стовпці, 2-й графік)\n",
    "plt.scatter(\n",
    "    X_class[:, 0],  # Дані для осі X (перша ознака)\n",
    "    X_class[:, 1],  # Дані для осі Y (друга ознака)\n",
    "    c=y_class,  # Колір точок залежить від класу\n",
    "    s=10,  # Розмір точок\n",
    "    cmap='viridis',  # Кольорова шкала\n",
    "    marker='o',  # Форма маркерів (точки)\n",
    "    alpha=0.5  # Прозорість точок\n",
    ")  \n",
    "plt.title('Дані для класифікації')  # Додаємо заголовок\n",
    "plt.xlabel('X1')  # Додаємо підпис осі X\n",
    "plt.ylabel('X2')  # Додаємо підпис осі Y\n",
    "\n",
    "plt.show()  # Відображаємо графіки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Побудова та навчання моделей\n",
    "\n",
    "# --- Регресія ---\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(  # Розділяємо дані на тренувальні та тестові\n",
    "    X_reg,  # Вхідні дані\n",
    "    y_reg,  # Вихідні дані\n",
    "    test_size=0.2,  # Розмір тестової вибірки (20%)\n",
    "    train_size=None,  # Розмір тренувальної вибірки (None - автоматично)\n",
    "    random_state=42,  # Фіксоване значення для генератора випадкових чисел\n",
    "    shuffle=True,  # Чи перемішувати дані перед розділенням\n",
    "    stratify=None  # Розподіл класів (для класифікації)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = keras.Sequential([  # Створюємо послідовну модель\n",
    "    keras.layers.Dense(\n",
    "        64,  # Кількість нейронів у шарі\n",
    "        activation='relu',  # Функція активації - ReLU\n",
    "        input_shape=(X_train_reg.shape[1],),  # Розмірність вхідних даних\n",
    "        # --- інші параметри ---\n",
    "        # use_bias=True,  # Чи використовувати зсув\n",
    "        # kernel_initializer='glorot_uniform',  # Ініціалізація ваг\n",
    "        # bias_initializer='zeros',  # Ініціалізація зсувів\n",
    "        # kernel_regularizer=None,  # Регуляризація ваг\n",
    "        # bias_regularizer=None,  # Регуляризація зсувів\n",
    "        # activity_regularizer=None,  # Регуляризація активацій\n",
    "        # kernel_constraint=None,  # Обмеження на ваги\n",
    "        # bias_constraint=None  # Обмеження на зсуви\n",
    "    ),  \n",
    "    keras.layers.Dense(32, activation='relu'),  # Другий шар з 32 нейронами, функція активації ReLU\n",
    "    keras.layers.Dense(1)  # Вихідний шар з 1 нейроном (для регресії)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg.fit(\n",
    "    X_train_reg,  # Тренувальні вхідні дані\n",
    "    y_train_reg,  # Тренувальні вихідні дані\n",
    "    batch_size=None,  # Розмір пакету даних (None - весь набір)\n",
    "    epochs=100,  # Кількість епох навчання\n",
    "    verbose=0,  # 0 - приховати інформацію про процес навчання, 1 - показувати прогрес, 2 - показувати лише епохи\n",
    "    callbacks=None,  # Список функцій зворотного виклику\n",
    "    validation_split=0.0,  # Частина тренувальних даних для валідації\n",
    "    validation_data=None,  # Дані для валідації\n",
    "    shuffle=True,  # Чи перемішувати дані перед кожною епохою\n",
    "    class_weight=None,  # Ваги для різних класів (для класифікації)\n",
    "    sample_weight=None,  # Ваги для різних зразків\n",
    "    initial_epoch=0,  # Початкова епоха\n",
    "    steps_per_epoch=None,  # Кількість кроків за епоху\n",
    "    validation_steps=None,  # Кількість кроків валідації\n",
    "    validation_batch_size=None,  # Розмір пакету даних для валідації\n",
    "    validation_freq=1,  # Частота валідації (кожні N епох)\n",
    "    #max_queue_size=10,   Максимальний розмір черги даних\n",
    "    #workers=1,   Кількість процесів для завантаження даних\n",
    "    #use_multiprocessing=False   Чи використовувати багатопроцесорність\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Класифікація ---\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(  # Розділяємо дані на тренувальні та тестові\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = keras.Sequential([  # Створюємо послідовну модель\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train_class.shape[1],)),  # Перший шар з 64 нейронами, функція активації ReLU\n",
    "    keras.layers.Dense(32, activation='relu'),  # Другий шар з 32 нейронами, функція активації ReLU\n",
    "    keras.layers.Dense(student_id + 2, activation='softmax')  \n",
    "    # Вихідний шар з кількістю нейронів, що дорівнює кількості класів, функція активації softmax (для класифікації)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Функція втрат для класифікації з цілочисельними мітками класів\n",
    "    optimizer='adam',  # Оптимізатор - Adam\n",
    "    metrics=['accuracy']  # Метрика для оцінки - точність (accuracy)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.fit(\n",
    "    X_train_class,  # Тренувальні вхідні дані\n",
    "    y_train_class,  # Тренувальні вихідні дані\n",
    "    epochs=100,  # Кількість епох навчання\n",
    "    verbose=1  # 0 - приховати інформацію про процес навчання\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Оцінка моделей\n",
    "\n",
    "# --- Регресія ---\n",
    "y_pred_reg = model_reg.predict(X_test_reg)  # Робимо передбачення на тестових даних\n",
    "mse = mean_squared_error(\n",
    "    y_test_reg,  # Справжні значення\n",
    "    y_pred_reg,  # Передбачені значення\n",
    "    # --- інші параметри ---\n",
    "    # sample_weight=None,  # Ваги для різних зразків\n",
    "    # multioutput='uniform_average'  # Як обробляти багатовимірні вихідні дані\n",
    ")  \n",
    "print(f'Mean Squared Error (Регресія): {mse}')  # Виводимо результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Класифікація ---\n",
    "y_pred_class = np.argmax(\n",
    "    model_class.predict(X_test_class),  # Передбачені ймовірності для кожного класу\n",
    "    axis=1  # Вісь, по якій визначається клас з найбільшою ймовірністю\n",
    ")  \n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)  # Обчислюємо точність\n",
    "print(f'Accuracy (Класифікація): {accuracy}')  # Виводимо результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(\n",
    "    y_test_class,  # Справжні мітки класів\n",
    "    y_pred_class,  # Передбачені мітки класів\n",
    "    # --- інші параметри ---\n",
    "    # labels=None,  # Список міток класів\n",
    "    # sample_weight=None,  # Ваги для різних зразків\n",
    "    # normalize=None  # Нормалізація матриці (None, 'true', 'pred', 'all')\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))  # Створюємо фігуру для графіка розміром 8x6 дюймів\n",
    "sns.heatmap(\n",
    "    cm,  # Матриця плутанини\n",
    "    annot=True,  # Показувати значення в комірках\n",
    "    fmt='d',  # Формат значень (цілі числа)\n",
    "    cmap='Blues',  # Кольорова шкала\n",
    "    # --- інші параметри ---\n",
    "    # linewidths=0,  # Ширина ліній між комірками\n",
    "    # linecolor='white',  # Колір ліній між комірками\n",
    "    # cbar=True,  # Показувати кольорову шкалу\n",
    "    # square=False,  # Чи робити комірки квадратними\n",
    "    # xticklabels='auto',  # Мітки для осі X\n",
    "    # yticklabels='auto',  # Мітки для осі Y\n",
    "    # mask=None  # Маска для приховування деяких комірок\n",
    ")  \n",
    "plt.title('Матриця плутанини')  # Додаємо заголовок\n",
    "plt.xlabel('Передбачені класи')  # Додаємо підпис осі X\n",
    "plt.ylabel('Справжні класи')  # Додаємо підпис осі Y\n",
    "plt.show()  # Відображаємо графік"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
